# -*- coding:utf-8 -*-

from sklearn import svm
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report
import xgboost as xgb
from tensorflow.keras.layers import *
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras import optimizers



def do_svm(x_train, x_test, y_train, y_test):
    '''
        model = svm
    Args:
        x_train:
        x_test:
        y_train:
        y_test:

    Returns:

    '''
    clf = svm.SVC(kernel='linear', C=1.0)
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    print(classification_report(y_test, y_pred))

def do_xgboost(x_train, x_test, y_train, y_test):
    '''
        model = xgboost
    Args:
        x_train:
        x_test:
        y_train:
        y_test:

    Returns:

    '''
    xgb_model = xgb.XGBClassifier().fit(x_train, y_train)
    y_pred = xgb_model.predict(x_test)
    print(classification_report(y_test, y_pred))


def do_mlp(x_train, x_test, y_train, y_test):
    '''
        model = mlp
    Args:
        x_train:
        x_test:
        y_train:
        y_test:

    Returns:

    '''
    clf = MLPClassifier(solver='lbfgs',
                        alpha=1e-5,
                        hidden_layer_sizes = (10, 4),
                        random_state = 1)
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    print(classification_report(y_test, y_pred))
    #print metrics.confusion_matrix(y_test, y_pred)

def do_cnn(trainX, testX, trainY, testY):
    '''
        model = cnn
    Args:
        trainX:
        testX:
        trainY:
        testY:

    Returns:

    '''
    # Converting labels to binary vectors
    trainY = to_categorical(trainY, num_classes=4)
    testY = to_categorical(testY, num_classes=4)
    # Building convolutional network
    input_data = Input(shape=[32, 32,1, ], name='input')
    cnn = Conv2D(input_shape=(x_train.shape[1], x_train.shape[2], x_train.shape[3]), filters=32, kernel_size=(3, 3),
                 strides=(1,1), padding='valid', activation='relu')(input_data)
    cnn = MaxPool2D(pool_size=(2, 2))(cnn)
    batchNormalization = BatchNormalization()(cnn)
    flatten = Flatten()(batchNormalization)
    dense = Dense(16, activation='tanh')(flatten)
    dropout = Dropout(0.2)(dense)
    output = Dense(4, activation='softmax')(dropout)

    model = Model(inputs=[input_data], outputs=[output])
    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.0001), metrics=['accuracy'])
    model.summary()

    model.fit(trainX, trainY, validation_data=(testX, testY),
              epochs=10, batch_size=128, verbose=1)

    score, acc = model.evaluate(testX, testY, batch_size=10)
    print('Test score:', score)
    print('Test accuracy:', acc)



def do_cnn_1d(trainX, testX, trainY, testY, vocab):
    '''
        model = text cnn
    Args:
        trainX:
        testX:
        trainY:
        testY:

    Returns:

    '''
    # Converting labels to binary vectors
    trainY = to_categorical(trainY, num_classes=4)
    testY = to_categorical(testY, num_classes=4)


    # Building convolutional network
    input_data = Input(shape=(1000, ))    # (maxlen, )
    embedding = Embedding(input_dim=len(vocab)+1, output_dim=64)(input_data)
    cnn1 = Conv1D(filters=32, kernel_size=3, strides=1, padding='valid')(embedding)
    cnn1 = MaxPool1D(4, padding='valid')(cnn1)
    # cnn2 = Conv1D(filters=32, kernel_size=4, strides=1, padding='valid')(embedding)
    # cnn2 = MaxPool1D(4, padding='valid')(cnn2)
    # cnn3 = Conv1D(filters=32, kernel_size=5, strides=1, padding='valid')(embedding)
    # cnn3 = MaxPool1D(4, padding='valid')(cnn3)

    # cnn = Concatenate(axis=-1)([cnn1, cnn2, cnn3])

    flatten = Flatten()(cnn1)

    dropout = Dropout(0.2)(flatten)
    # batchNormalization = BatchNormalization()(dropout1)
    # dense1 = Dense(16, activation='relu')(batchNormalization)
    # dropout2 = Dropout(0.2)(dense1)
    output_data = Dense(4, activation='softmax')(dropout)

    model = Model(inputs=[input_data], outputs=[output_data])
    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.0001), metrics=['accuracy'])
    model.summary()

    model.fit(trainX, trainY, validation_data=(testX, testY),
              epochs=10, batch_size=128, verbose=1)
    # y_pred = model.predict(x_test)
    # y_pred_idx = [1 if prob[0] > 0.5 else 0 for prob in y_pred]
    # print(f1_score(y_test, y_pred_idx))
    # print(confusion_matrix(y_test, y_pred_idx))
    # 评估误差和准确率
    # result = model.predict(testX)  # 预测的是类别，结果就是类别号
    # result_labels = np.argmax(result, axis=1)
    # y_predict = list(map(str, result_labels))
    # print('准确率', metrics.accuracy_score(testY, result_labels))
    # print('平均f1-score:', metrics.f1_score(testY, result_labels, average='weighted'))
    score, acc = model.evaluate(testX, testY, batch_size=10)
    print('Test score:', score)
    print('Test accuracy:', acc)


if __name__ == "__main__":
    print("Hello malware")
    print("text feature and cnn")
    x_train, x_test, y_train, y_test = get_feature_pe_picture()

    do_cnn(x_train, x_test, y_train, y_test)
    # x_train, x_test, y_train, y_test = get_feature_text()
    # print(x_train.shape)
    # print(y_train.shape)
    # do_cnn_1d(x_train, x_test, y_train, y_test, vocab)
    # do_svm(x_train, x_test, y_train, y_test)    #95
    # do_xgboost(x_train, x_test, y_train, y_test)    # 96
    # do_mlp(x_train, x_test, y_train, y_test)    #  89

    # x_train, x_test, y_train, y_test = get_feature_pe_picture()
    # get list

    # do_cnn_1d(x_train, x_test, y_train, y_test)

"""
    print "text feature and xgboost"
    x_train, x_test, y_train, y_test=get_feature_text()
    do_xgboost(x_train, x_test, y_train, y_test)

    print "text feature and svm"
    x_train, x_test, y_train, y_test=get_feature_text()
    do_svm(x_train, x_test, y_train, y_test)

    print "text feature and mlp"
    x_train, x_test, y_train, y_test=get_feature_text()
    do_mlp(x_train, x_test, y_train, y_test)
    """

