# -*- coding:utf-8 -*-

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.feature_extraction.text import TfidfTransformer
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences



def load_files_from_dir(dir):
    '''  加载4个文件夹下面的4类恶意程序文档  敏感词采用空格替代

    Args:
        dir:

    Returns:

    '''
    import glob
    files=glob.glob(dir)
    result = []
    for file in files:
        #print "Load file %s" % file
        with open(file) as f:
            lines=f.readlines()
            lines_to_line=" ".join(lines)
            lines_to_line = re.sub(r"[APT|Crypto|Locker|Zeus]", ' ', lines_to_line,flags=re.I)
            result.append(lines_to_line)
    return result

def load_files():
    '''

    Returns:

    '''
    malware_class=['APT1','Crypto','Locker','Zeus']
    x=[]
    y=[]
    for i,family in enumerate(malware_class):
        dir = "../../dataset/MalwareTrainingSets-master/trainingSets/%s/*" % family
        print("Load files from %s index %d" % (dir,i))
        v = load_files_from_dir(dir)
        x += v
        y += [i]*len(v)     # [1,2,3,4]
    print("Loaded files %d" % len(x))
    return x, y

def get_feature_text():
    '''
        文本特征
    Returns:

    '''
    x, y = load_files()
    max_features = 1000
    vectorizer = CountVectorizer(
            decode_error='ignore',
            ngram_range=(2, 2),
            strip_accents='ascii',
            max_features=max_features,
            stop_words='english',
            max_df=1.0,
            min_df=1,
            token_pattern=r'\b\w+\b',    # 以单词为单位切分
            binary=True)
    print(vectorizer)
    x = vectorizer.fit_transform(x)
    transformer = TfidfTransformer(smooth_idf=False)
    x = transformer.fit_transform(x)

    # 非常重要 稀疏矩阵转换成矩阵   以数据为准
    x = x.toarray()

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)
    y_train = np.array(y_train)
    y_test = np.array(y_test)
    return x_train, x_test, y_train, y_test

def get_feature_token():
    x, y = load_files()
    # x = np.array(x)
    # y = np.array(y)

    tokenizer = Tokenizer()  # 创建一个Tokenizer对象
    # fit_on_texts函数可以将输入的文本中的每个词编号，编号是根据词频的，词频越大，编号越小
    tokenizer.fit_on_texts(x)
    vocab = tokenizer.word_index  # 得到每个词的编号

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
    # 将每个样本中的每个词转换为数字列表，使用每个词的编号进行编号
    x_train_word_ids = tokenizer.texts_to_sequences(x_train)
    x_test_word_ids = tokenizer.texts_to_sequences(x_test)
    # 序列模式
    # pad sequences   find max len
    # max_sequence_len = max([len(x) for x in x_train_word_ids])
    # input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))
    # 每条样本长度不唯一，将每条样本的长度设置一个固定值
    x_train_padded_seqs = pad_sequences(x_train_word_ids, maxlen=1000)  # 将超过固定值的部分截掉，不足的在最前面用0填充
    x_test_padded_seqs = pad_sequences(x_test_word_ids, maxlen=1000)
    y_train = np.array(y_train)
    y_test = np.array(y_test)

    return x_train_padded_seqs, x_test_padded_seqs, y_train, y_test, vocab

def save_image(data,index):
    '''  将数据保存为图片

    Args:
        data:
        index:

    Returns:

    '''
    from PIL import Image  #  处理图片
    new_im = Image.fromarray(data)
    new_im.save("../dataset/MalwareTrainingSets-master/%s.bmp" % index)

def get_feature_pe_picture():
    '''  以图形方式获取特征训练模型

    Returns:

    '''
    #加载原始文件
    x, y = load_files()
    max_features = 1024
    vectorizer = CountVectorizer(
            decode_error='ignore',
            ngram_range=(2, 2),
            strip_accents='ascii',
            max_features=max_features,
            stop_words='english',
            max_df=1.0,
            min_df=1,
            dtype=np.int,
            token_pattern=r'\b\w+\b',
            binary=False)
    print(vectorizer)
    x = vectorizer.fit_transform(x)
    #非常重要 稀疏矩阵转换成矩阵
    x = x.toarray()
    x_pic = []
    for i in range(4762):
        #将形状为(1024，1)的向量转化成（32，32）的矩阵
        pic = np.reshape(x[i],(32,32,1))
        x_pic.append(pic)
        #save_image(pic,i)
    #随机分配训练和测试集合
    x_train, x_test, y_train, y_test = train_test_split(x_pic, y, test_size=0.4)
    x_train = np.array(x_train)
    x_test = np.array(x_test)
    y_train = np.array(y_train)
    y_test = np.array(y_test)

    return x_train, x_test, y_train, y_test

